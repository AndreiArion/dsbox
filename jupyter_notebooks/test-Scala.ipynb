{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Spark with Scala kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rdd = sc.parallelize(Array.range(1, 100),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val a = rdd.map(x => 2*x).reduce(_ + _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9900\n"
     ]
    }
   ],
   "source": [
    "println(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "// To make some of the examples work we will also need RDD\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VertexProperty()\n",
    "case class UserProperty(val name: String) extends VertexProperty\n",
    "case class ProductProperty(val name: String, val price: Double) extends VertexProperty\n",
    "// The graph might then have the type:\n",
    "var graph: Graph[VertexProperty, String] = null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Create an RDD for the vertices\n",
    "val users: RDD[(VertexId, (String, String))] =\n",
    "  sc.parallelize(Array((3L, (\"rxin\", \"student\")), (7L, (\"jgonzal\", \"postdoc\")),\n",
    "                       (5L, (\"franklin\", \"prof\")), (2L, (\"istoica\", \"prof\"))))\n",
    "// Create an RDD for edges\n",
    "val relationships: RDD[Edge[String]] =\n",
    "  sc.parallelize(Array(Edge(3L, 7L, \"collab\"),    Edge(5L, 3L, \"advisor\"),\n",
    "                       Edge(2L, 5L, \"colleague\"), Edge(5L, 7L, \"pi\")))\n",
    "// Define a default user in case there are relationship with missing user\n",
    "val defaultUser = (\"John Doe\", \"Missing\")\n",
    "// Build the initial Graph\n",
    "val graph = Graph(users, relationships, defaultUser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val graph: Graph[(String, String), String] // Constructed from above\n",
    "// Count all users which are postdocs\n",
    "graph.vertices.filter { case (id, (name, pos)) => pos == \"postdoc\" }.count\n",
    "// Count all the edges where src > dst\n",
    "graph.edges.filter(e => e.srcId > e.dstId).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.edges.filter { case Edge(src, dst, prop) => src > dst }.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "istoica is the colleague of franklin\n",
      "rxin is the collab of jgonzal\n",
      "franklin is the advisor of rxin\n",
      "franklin is the pi of jgonzal\n"
     ]
    }
   ],
   "source": [
    "val facts: RDD[String] =\n",
    "  graph.triplets.map(triplet =>\n",
    "    triplet.srcAttr._1 + \" is the \" + triplet.attr + \" of \" + triplet.dstAttr._1)\n",
    "facts.collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "istoica is the colleague of franklin\n",
      "rxin is the collab of jgonzal\n",
      "peter is the student of John Doe\n",
      "franklin is the colleague of John Doe\n",
      "franklin is the advisor of rxin\n",
      "franklin is the pi of jgonzal\n",
      "(4,(peter,student))\n",
      "(3,(rxin,student))\n",
      "(7,(jgonzal,postdoc))\n",
      "(5,(franklin,prof))\n",
      "(2,(istoica,prof))\n",
      "istoica is the colleague of franklin\n",
      "rxin is the collab of jgonzal\n",
      "franklin is the advisor of rxin\n",
      "franklin is the pi of jgonzal\n"
     ]
    }
   ],
   "source": [
    "// Create an RDD for the vertices\n",
    "val users: RDD[(VertexId, (String, String))] =\n",
    "  sc.parallelize(Array((3L, (\"rxin\", \"student\")), (7L, (\"jgonzal\", \"postdoc\")),\n",
    "                       (5L, (\"franklin\", \"prof\")), (2L, (\"istoica\", \"prof\")),\n",
    "                       (4L, (\"peter\", \"student\"))))\n",
    "// Create an RDD for edges\n",
    "val relationships: RDD[Edge[String]] =\n",
    "  sc.parallelize(Array(Edge(3L, 7L, \"collab\"),    Edge(5L, 3L, \"advisor\"),\n",
    "                       Edge(2L, 5L, \"colleague\"), Edge(5L, 7L, \"pi\"),\n",
    "                       Edge(4L, 0L, \"student\"),   Edge(5L, 0L, \"colleague\")))\n",
    "// Define a default user in case there are relationship with missing user\n",
    "val defaultUser = (\"John Doe\", \"Missing\")\n",
    "// Build the initial Graph\n",
    "val graph = Graph(users, relationships, defaultUser)\n",
    "// Notice that there is a user 0 (for which we have no information) connected to users\n",
    "// 4 (peter) and 5 (franklin).\n",
    "graph.triplets.map(\n",
    "    triplet => triplet.srcAttr._1 + \" is the \" + triplet.attr + \" of \" + triplet.dstAttr._1\n",
    "  ).collect.foreach(println(_))\n",
    "// Remove missing vertices as well as the edges to connected to them\n",
    "val validGraph = graph.subgraph(vpred = (id, attr) => attr._2 != \"Missing\")\n",
    "// The valid subgraph will disconnect users 4 and 5 by removing user 0\n",
    "validGraph.vertices.collect.foreach(println(_))\n",
    "validGraph.triplets.map(\n",
    "    triplet => triplet.srcAttr._1 + \" is the \" + triplet.attr + \" of \" + triplet.dstAttr._1\n",
    "  ).collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Run Connected Components\n",
    "val ccGraph = graph.connectedComponents() // No longer contains missing field\n",
    "// Remove missing vertices as well as the edges to connected to them\n",
    "val validGraph = graph.subgraph(vpred = (id, attr) => attr._2 != \"Missing\")\n",
    "// Restrict the answer to the valid subgraph\n",
    "val validCCGraph = ccGraph.mask(validGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19,56.52173913043478)\n",
      "(39,62.888888888888886)\n",
      "(34,70.07142857142857)\n",
      "(4,48.5)\n",
      "(71,87.44444444444444)\n",
      "(66,79.0)\n",
      "(80,84.2)\n",
      "(65,78.5)\n",
      "(11,66.17647058823529)\n",
      "(14,70.5)\n",
      "(35,58.1764705882353)\n",
      "(24,61.333333333333336)\n",
      "(37,72.46153846153847)\n",
      "(1,48.5)\n",
      "(74,80.25)\n",
      "(63,80.625)\n",
      "(89,90.0)\n",
      "(17,57.44444444444444)\n",
      "(18,64.05555555555556)\n",
      "(12,54.7)\n",
      "(38,59.111111111111114)\n",
      "(20,60.72727272727273)\n",
      "(90,98.0)\n",
      "(94,98.0)\n",
      "(41,68.54545454545455)\n",
      "(21,49.0)\n",
      "(77,83.5)\n",
      "(53,73.6923076923077)\n",
      "(22,60.6)\n",
      "(25,61.857142857142854)\n",
      "(46,75.3)\n",
      "(59,78.84615384615384)\n",
      "(62,92.14285714285714)\n",
      "(93,97.0)\n",
      "(33,64.0)\n",
      "(23,59.714285714285715)\n",
      "(40,65.375)\n",
      "(6,60.3)\n",
      "(67,73.0)\n",
      "(69,86.0)\n",
      "(3,45.4)\n",
      "(85,86.33333333333333)\n",
      "(58,72.4)\n",
      "(60,81.72727272727273)\n",
      "(86,96.0)\n",
      "(91,94.0)\n",
      "(31,66.72222222222223)\n",
      "(26,64.6923076923077)\n",
      "(5,53.625)\n",
      "(2,58.65)\n",
      "(13,50.92307692307692)\n",
      "(96,98.0)\n",
      "(52,79.0)\n",
      "(81,88.75)\n",
      "(16,47.5)\n",
      "(55,79.25)\n",
      "(82,86.0)\n",
      "(28,73.91666666666667)\n",
      "(29,56.529411764705884)\n",
      "(79,88.75)\n",
      "(54,76.0)\n",
      "(30,60.2)\n",
      "(50,69.54545454545455)\n",
      "(36,70.125)\n",
      "(92,99.0)\n",
      "(64,82.11111111111111)\n",
      "(57,68.2)\n",
      "(51,79.0)\n",
      "(75,84.0)\n",
      "(45,71.85714285714286)\n",
      "(72,85.66666666666667)\n",
      "(70,78.25)\n",
      "(9,48.833333333333336)\n",
      "(49,70.15384615384616)\n",
      "(78,82.0)\n",
      "(43,65.5)\n",
      "(10,51.541666666666664)\n",
      "(84,90.4)\n",
      "(61,79.53333333333333)\n",
      "(56,80.66666666666667)\n",
      "(15,62.04761904761905)\n",
      "(47,74.2)\n",
      "(76,86.66666666666667)\n",
      "(95,96.0)\n",
      "(48,60.125)\n",
      "(73,82.42857142857143)\n",
      "(32,65.94444444444444)\n",
      "(27,64.66666666666667)\n",
      "(0,58.285714285714285)\n",
      "(42,74.75)\n",
      "(7,49.9)\n",
      "(8,63.89473684210526)\n",
      "(44,69.38095238095238)\n",
      "(88,92.0)\n",
      "(68,88.4)\n"
     ]
    }
   ],
   "source": [
    "// Import random graph generation library\n",
    "import org.apache.spark.graphx.util.GraphGenerators\n",
    "// Create a graph with \"age\" as the vertex property.  Here we use a random graph for simplicity.\n",
    "val graph: Graph[Double, Int] =\n",
    "  GraphGenerators.logNormalGraph(sc, numVertices = 100).mapVertices( (id, _) => id.toDouble )\n",
    "// Compute the number of older followers and their total age\n",
    "val olderFollowers: VertexRDD[(Int, Double)] = graph.aggregateMessages[(Int, Double)](\n",
    "  triplet => { // Map Function\n",
    "    if (triplet.srcAttr > triplet.dstAttr) {\n",
    "      // Send message to destination vertex containing counter and age\n",
    "      triplet.sendToDst(1, triplet.srcAttr)\n",
    "    }\n",
    "  },\n",
    "  // Add counter and age\n",
    "  (a, b) => (a._1 + b._1, a._2 + b._2) // Reduce Function\n",
    ")\n",
    "// Divide total age by number of older followers to get average age of older followers\n",
    "val avgAgeOfOlderFollowers: VertexRDD[Double] =\n",
    "  olderFollowers.mapValues( (id, value) => value match { case (count, totalAge) => totalAge / count } )\n",
    "// Display the results\n",
    "avgAgeOfOlderFollowers.collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-1.5.2-bin-hadoop2.6/lib/spark-assembly-1.5.2-hadoop2.6.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      ":: resolving dependencies :: com.ibm.spark#spark-kernel;working\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-csv_2.10;1.3.0 in central\n",
      "\tfound org.apache.commons#commons-csv;1.1 in central\n",
      "\tfound com.univocity#univocity-parsers;1.5.1 in central\n",
      "\tfound org.scoverage#scalac-scoverage-runtime_2.10;1.1.0 in central\n",
      "\tfound org.scoverage#scalac-scoverage-plugin_2.10;1.1.0 in central\n",
      "\tfound org.apache.spark#spark-core_2.10;1.5.0 in central\n",
      "\tfound com.google.guava#guava;14.0.1 in central\n",
      "\tfound com.google.code.findbugs#jsr305;1.3.9 in central\n",
      "\tfound javax.inject#javax.inject;1 in central\n",
      "\tfound org.apache.avro#avro-mapred;1.7.7 in central\n",
      "\tfound org.apache.avro#avro-ipc;1.7.7 in central\n",
      "\tfound org.apache.avro#avro;1.7.7 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.1.7 in central\n",
      "\tfound org.apache.commons#commons-compress;1.4.1 in central\n",
      "\tfound org.tukaani#xz;1.0 in central\n",
      "\tfound com.twitter#chill_2.10;0.5.0 in central\n",
      "\tfound com.twitter#chill-java;0.5.0 in central\n",
      "\tfound com.esotericsoftware.kryo#kryo;2.21 in central\n",
      "\tfound com.esotericsoftware.reflectasm#reflectasm;1.07 in central\n",
      "\tfound com.esotericsoftware.minlog#minlog;1.2 in central\n",
      "\tfound org.objenesis#objenesis;1.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-client;2.2.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-common;2.2.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;2.2.0 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound org.apache.commons#commons-math;2.1 in central\n",
      "\tfound xmlenc#xmlenc;0.52 in central\n",
      "\tfound commons-httpclient#commons-httpclient;3.1 in central\n",
      "\tfound commons-codec#commons-codec;1.10 in central\n",
      "\tfound commons-io#commons-io;2.1 in central\n",
      "\tfound commons-net#commons-net;2.2 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in central\n",
      "\tfound commons-configuration#commons-configuration;1.6 in central\n",
      "\tfound commons-collections#commons-collections;3.2.1 in central\n",
      "\tfound commons-digester#commons-digester;1.8 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.7.0 in central\n",
      "\tfound commons-beanutils#commons-beanutils-core;1.8.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;2.2.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-hdfs;2.2.0 in central\n",
      "\tfound org.mortbay.jetty#jetty-util;6.1.26 in central\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-app;2.2.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-common;2.2.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-common;2.2.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-api;2.2.0 in central\n",
      "\tfound org.slf4j#slf4j-log4j12;1.7.10 in central\n",
      "\tfound com.google.inject#guice;3.0 in central\n",
      "\tfound aopalliance#aopalliance;1.0 in central\n",
      "\tfound org.sonatype.sisu.inject#cglib;2.2.1-v20090111 in central\n",
      "\tfound com.sun.jersey.jersey-test-framework#jersey-test-framework-grizzly2;1.9 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.9 in central\n",
      "\tfound asm#asm;3.2 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.9 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound stax#stax-api;1.0.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.2 in central\n",
      "\tfound javax.activation#activation;1.1 in central\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound com.sun.jersey.contribs#jersey-guice;1.9 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-client;2.2.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-core;2.2.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-server-common;2.2.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-shuffle;2.2.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-jobclient;2.2.0 in central\n",
      "\tfound org.apache.spark#spark-launcher_2.10;1.5.0 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.spark#spark-network-common_2.10;1.5.0 in central\n",
      "\tfound io.netty#netty-all;4.0.29.Final in central\n",
      "\tfound org.apache.spark#spark-network-shuffle_2.10;1.5.0 in central\n",
      "\tfound org.apache.spark#spark-unsafe_2.10;1.5.0 in central\n",
      "\tfound net.java.dev.jets3t#jets3t;0.7.1 in central\n",
      "\tfound org.apache.curator#curator-recipes;2.4.0 in central\n",
      "\tfound org.apache.curator#curator-framework;2.4.0 in central\n",
      "\tfound org.apache.curator#curator-client;2.4.0 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.4.5 in central\n",
      "\tfound jline#jline;0.9.94 in central\n",
      "\tfound org.eclipse.jetty.orbit#javax.servlet;3.0.0.v201112011016 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.3.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.4.1 in central\n",
      "\tfound org.slf4j#jul-to-slf4j;1.7.10 in central\n",
      "\tfound org.slf4j#jcl-over-slf4j;1.7.10 in central\n",
      "\tfound com.ning#compress-lzf;1.0.3 in central\n",
      "\tfound net.jpountz.lz4#lz4;1.3.0 in central\n",
      "\tfound org.roaringbitmap#RoaringBitmap;0.4.5 in central\n",
      "\tfound com.typesafe.akka#akka-remote_2.10;2.3.11 in central\n",
      "\tfound com.typesafe.akka#akka-actor_2.10;2.3.11 in central\n",
      "\tfound com.typesafe#config;1.2.1 in central\n",
      "\tfound io.netty#netty;3.8.0.Final in central\n",
      "\tfound org.uncommons.maths#uncommons-maths;1.2.2a in central\n",
      "\tfound com.typesafe.akka#akka-slf4j_2.10;2.3.11 in central\n",
      "\tfound org.json4s#json4s-jackson_2.10;3.2.10 in central\n",
      "\tfound org.json4s#json4s-core_2.10;3.2.10 in central\n",
      "\tfound org.json4s#json4s-ast_2.10;3.2.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.6 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.4.4 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.4.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.4.4 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.9 in central\n",
      "\tfound org.apache.mesos#mesos;0.21.1 in central\n",
      "\tfound com.clearspring.analytics#stream;2.7.0 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;3.1.2 in central\n",
      "\tfound io.dropwizard.metrics#metrics-jvm;3.1.2 in central\n",
      "\tfound io.dropwizard.metrics#metrics-json;3.1.2 in central\n",
      "\tfound io.dropwizard.metrics#metrics-graphite;3.1.2 in central\n",
      "\tfound com.fasterxml.jackson.module#jackson-module-scala_2.10;2.4.4 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.4.4 in central\n",
      "\tfound org.apache.ivy#ivy;2.4.0 in central\n",
      "\tfound oro#oro;2.0.8 in central\n",
      "\tfound org.tachyonproject#tachyon-client;0.7.1 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound org.tachyonproject#tachyon-underfs-hdfs;0.7.1 in central\n",
      "\tfound org.tachyonproject#tachyon-underfs-local;0.7.1 in central\n",
      "\tfound net.razorvine#pyrolite;4.4 in central\n",
      "\tfound net.sf.py4j#py4j;0.8.2.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-server-nodemanager;2.2.0 in central\n",
      "\tfound org.jboss.netty#netty;3.2.2.Final in central\n",
      "\tfound org.apache.spark#spark-sql_2.10;1.5.0 in central\n",
      "\tfound org.apache.spark#spark-catalyst_2.10;1.5.0 in central\n",
      "\tfound org.codehaus.janino#janino;2.7.8 in central\n",
      "\tfound org.codehaus.janino#commons-compiler;2.7.8 in central\n",
      "\tfound org.apache.parquet#parquet-column;1.7.0 in central\n",
      "\tfound org.apache.parquet#parquet-common;1.7.0 in central\n",
      "\tfound org.apache.parquet#parquet-encoding;1.7.0 in central\n",
      "\tfound org.apache.parquet#parquet-generator;1.7.0 in central\n",
      "\tfound org.apache.parquet#parquet-hadoop;1.7.0 in central\n",
      "\tfound org.apache.parquet#parquet-format;2.3.0-incubating in central\n",
      "\tfound org.apache.parquet#parquet-jackson;1.7.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      "downloading https://repo1.maven.org/maven2/com/databricks/spark-csv_2.10/1.3.0/spark-csv_2.10-1.3.0.jar ...\n",
      "\t[SUCCESSFUL ] com.databricks#spark-csv_2.10;1.3.0!spark-csv_2.10.jar (55355ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-csv/1.1/commons-csv-1.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-csv;1.1!commons-csv.jar (8339ms)\n",
      "downloading https://repo1.maven.org/maven2/com/univocity/univocity-parsers/1.5.1/univocity-parsers-1.5.1.jar ...\n",
      "\t[SUCCESSFUL ] com.univocity#univocity-parsers;1.5.1!univocity-parsers.jar (27398ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scoverage/scalac-scoverage-runtime_2.10/1.1.0/scalac-scoverage-runtime_2.10-1.1.0.jar ...\n",
      "\t[SUCCESSFUL ] org.scoverage#scalac-scoverage-runtime_2.10;1.1.0!scalac-scoverage-runtime_2.10.jar (661ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scoverage/scalac-scoverage-plugin_2.10/1.1.0/scalac-scoverage-plugin_2.10-1.1.0.jar ...\n",
      "\t[SUCCESSFUL ] org.scoverage#scalac-scoverage-plugin_2.10;1.1.0!scalac-scoverage-plugin_2.10.jar (20807ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-core_2.10/1.5.0/spark-core_2.10-1.5.0.jar ...\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql_2.10/1.5.0/spark-sql_2.10-1.5.0.jar ...\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar ...\n"
     ]
    }
   ],
   "source": [
    "%AddDeps com.databricks spark-csv_2.10 1.3.0 --transitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "val sqlContext = new SQLContext(sc)\n",
    "val bankScala = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"delimiter\",\";\").option(\"inferSchema\", \"true\").load(\"/vagrant/zeppelin_notebooks/bank.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bankScala.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bankScala.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bankScala.describe().show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10 (Spark)",
   "language": "",
   "name": "scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
